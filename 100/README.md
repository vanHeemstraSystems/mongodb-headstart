# 100 - Introduction

I tend to have about a dozen or so little tech projects loaded onto my machine at any given time — little web app ideas that I work on and stop, an occasional hackathon entry, something I’ve built working through a tutorial, a few experiments. I’ve gotten to really enjoy working with MongoDB over the past year, and I’ll usually add that in as a persistence layer when appropriate. One thing I’ve realized working off of a local MongoDB development server is all of those databases can accumulate, making it a little scary to experiment with different server configurations and deployment setups for one app. Indeed, one of the challenges during local development is constantly changing and reconfiguring the database as the project evolves, and safely dropping, rebuilding, and reseeding databases.

One way I’ve found to add some flexibility to the database layer when developing locally is using Docker to run a database server that’s dedicated to the project. By leveraging volumes with Docker, we can quickly configure, change, and migrate a database, making it easier to keep database configurations consistent in higher environments. Likewise, this setup can also simplify creating and tearing down pre-populated databases for automated tests.

In this article, I’ll show you my Docker setup for container configurations and scripts to run a project’s MongoDB server in a Docker container. We’ll add some configuration files for a local database, then go on to seed some data, as well as simple commands to tear down and rebuild the database.

## Some terms and concepts

Before we set up our Docker configuration, I’d like to go over some concepts that will help clarify the steps we go through.

**Docker** is a platform for creating virtual environments called **containers**. I think of a container as a tiny, isolated server that is given just enough resources to run a single, specific application or a task. Docker containers run on a **host machine** which is a physical piece of hardware, e.g. my computer if I’m developing locally, or the physical server in a public deployment. Docker containers come from **images** which are basically a blueprint for creating the container: at a minimum, an image will describe an operating system to run in the container, and it will usually also have packages and libraries installed to help an application accomplish a task.

One of the most powerful aspects of Docker is that we can layer different pre-configured images, then add our own configurations and artifacts. Hundreds of different vendors and platforms make official Docker images available for their products — different Linux flavors, different Windows products, MySQL, Postgres, SQL Server — and indeed it’s a great way to play with all sorts of different technologies. For our purposes, there is [an official MongoDB image](https://hub.docker.com/_/mongo) that we’ll use to run a containerized database.

Once an image is defined, we can run the container on the host. When a container is run, usually we specify ports and network settings that are opened so that data can pass from the host machine to the container, or from container to container. We would also want to pass environment configuration settings to the container, for example a connection string so that the container can connect to an external database. Docker provides us with a variety of commands including one to run the container as a background process (**“detached mode”**), or with a command to be able to enter the container through the terminal and inspect its contents (```docker exec```, which we'll use later on).

One advantage of containers is that they are ephemeral — all you need is a command ```$ docker run <...>``` and the container will start with all of the components defined by its Docker image. When we don't need it, another command will make the container shut down and disappear. This helps developers to consistently deploy an application to different environments and to be sure it's most likely to run the exact same way. However, any data that is saved inside of the container will also disappear once the container is stopped. To handle persisted data, Docker uses **volumes** to connect data or files from the host machine to the container when it is running, as well as to write data from the container back to the host. 

There are two types: The first, **bind mounts** are directories that the developer manages directly. For example, if I want a directory of scripts available in the container, I can bind my local scripts directory in a project's source code to a directory in the container. The second type are called **volumes** and these are resources that are managed by Docker and live in a special location in the host machine's file system that we don't necessarily need to access directly. The advantage of using Docker volumes is that it gives us some special commands to create, manage, and dispose of persistent data that is consumed by a Docker container.

Lastly, **Docker Compose** is a feature that allows you to manage multiple Docker containers. Normally, it’s best practice for a Docker container to do only one thing, so for a web application, I might have one container to serve static files publicly, one container to handle the web API, and a third container to run a database server. Docker Compose allows us to create a YAML file, normally named ```docker-compose.yaml```, to define how multiple containers are configured and how they are allowed to communicate with each other.

So, taking all of these concepts together, we are going to use Docker to run our development database server in the following ways:

- We’ll use the official MongoDB **image** to create a **container** to have all of the functionality of a Mongo server without having to go through the cumbersome process of installing it on a development machine.
- We’ll **bind mount** a directory of startup scripts to a special directory on the Mongo container called ```/docker-entrypoint-initdb.d```. Any bash scripts or Javascript files in this directory are immediately run when the container is launched. We'll use this to create a MongoDB database and an application user.
- We’ll define a Docker **volume** so that the Mongo container can persist data on our host machine. If we ever need to remove all of the saved data and start over, we’ll have Docker commands available to do that.
- Lastly, we’ll use **Docker Compose** to define the Mongo container as a service. With a ```docker-compose.yaml``` file defined, an application service or other services can be added on for a more robust application.
